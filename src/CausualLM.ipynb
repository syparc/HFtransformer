{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9447ec-7aea-4179-97f7-50136be7d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인과적 언어 모델 - 생성 태스크\n",
    "# 허깅페이스에서는 이전 토큰에 의해 다음 토큰이 결정된다는 의미에서 인과적 생성 Causal generation이라고 부름\n",
    "# 국내에서는 대부분 생성 모델 generation model 이라고 지칭함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92d909f-de77-436e-aee1-45b1a2f6948c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97007b0f98da4cbf879c00cf6868a5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1e15fc4b8f4c0ea7487130306ed60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 05:40:59.478371: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-18 05:40:59.485227: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750225259.493387   42376 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750225259.495824   42376 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750225259.502081   42376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750225259.502088   42376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750225259.502090   42376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750225259.502090   42376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-18 05:40:59.504681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c46a9c7cbc748b59ce94beee9893f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    bos_token='</s>',  #kogpt2의 경우 특수토큰이 지정되지 않았으므로 지정해줌\n",
    "    eos_token='</s>',  #지정 여부는 단어사전을 출력하거나 tokenizer.json 파일을 열어서 확인 가능\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model #임베딩 레이어와 out_features 크기가 같음 - 입출력에서 동일한 단어사전을 공유해야 하기 때문임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2aae6c-e830-48d9-a753-e75f4935a61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36db9f7c375496dbc30cb92f1ae91c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86de56f739794f21805046989eab69e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kowikitext.py:   0%|          | 0.00/5.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for heegyu/kowikitext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/heegyu/kowikitext.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b630d510ca4eacae13c24cf404b2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kowikitext-20221001.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd64ef9ec014d009b5aab27d0d8df52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'revid', 'url', 'title', 'text'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'revid', 'url', 'title', 'text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 위키 데이터셋 사용 - 크기가 너무 크므로 일부만 사용한다\n",
    "from datasets import load_dataset\n",
    "\n",
    "split_dict = {\n",
    "    \"train\": \"train[:8000]\",\n",
    "    \"test\": \"train[8000:10000]\",\n",
    "    \"unused\": \"train[10000:]\"\n",
    "}\n",
    "dataset = load_dataset(\"heegyu/kowikitext\", split=split_dict)\n",
    "del dataset[\"unused\"]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668242fd-73ef-4243-8ca8-53e9853275e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4689171c9eac43a8a88f01d1a7dde4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914d36b225c94310ada628299271f05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델이 이해할 수 있도록 자연어 데이터 인코딩 - 타이틀과 내용을 개행문자로 구분하여 합친 후 토큰화\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda batch: tokenizer([f\"{ti}\\n{te}\" for ti,te in zip(batch[\"title\"],batch[\"text\"])]),\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72544a74-84af-4042-97d0-626ee92f50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 18365\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 2400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 전처리 수행 - 원본문서 끝에 <eos>를 넣고 최대 길이 512를 기준으로 그룹화\n",
    "max_length = 512\n",
    "def group_texts(batched_samples):\n",
    "    sample = {k: v[0] for k,v in batched_samples.items()}\n",
    "\n",
    "    if sample['input_ids'][-1] != tokenizer.eos_token_id:\n",
    "        for k in sample.keys():\n",
    "            sample[k].append(tokenizer.eos_token_id if k==\"input_ids\" else sample[k][-1])\n",
    "\n",
    "    result = {k: [v[i: i+max_length] for i in range(0, len(v), max_length)] for k,v in sample.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "grouped_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1,  # 하나의 샘플을 받아 여러 샘플을 출력하기 위한 설정\n",
    "    num_proc=2\n",
    ")\n",
    "\n",
    "print(len(grouped_dataset['train'][0]['input_ids']))\n",
    "print(grouped_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b5649c5-6fef-464f-ab13-461a4913587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 없는 labels 칼럼 추가\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\n",
    "sample = collator([grouped_dataset['train'][i] for i in range(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ac10659-583e-4934-b96a-c140f8c3a302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난해 7월 롯데백화점 본점 지하 1층 식품관에서 열린 ‘2018 롯데백화점 식품관’ 행사에서 선보인 ‘롯데백화점 식품관’ 브랜드는 총 4종이다.\n",
      "이번 행사에서 선보인 ‘롯데백화점 식품관’은 롯데백화점 식품관 내 식품관 중 가장 큰 규모다.\n",
      "롯데백화점 식품관은 식품관 내 식품관 내 식품관 중 가장 큰 규모다.\n",
      "롯데백화점 식품관은 식품관 내 식품관 내 식품\n"
     ]
    }
   ],
   "source": [
    "# 문장 생성\n",
    "inputs = tokenizer(\"지난해 7월 \", return_tensors='pt').to(model.device)\n",
    "\n",
    "outputs = model.generate(inputs.input_ids, max_new_tokens=100)\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffccd019-a3a5-4121-b05c-b6114f2185d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난해 7월 롯데백화점 본점 지하 1층 식품관에서 열린 ‘2018 롯데백화점 식품관’ 행사에서 선보인 ‘롯데백화점 식품관’ 브랜드는 총 4종이다.\n",
      "이번 행사에서 선보인 ‘롯데백화점 식품관’은 롯데백화점 식품관 내 식품관 중 가장 큰 규모다.\n",
      "롯데백화점 식품관은 식품관 내 식품관 내 식품관 중 가장 큰 규모다.\n",
      "롯데백화점 식품관은 식품관 내 식품관 내 식품\n"
     ]
    }
   ],
   "source": [
    "# 생성 작업을 하나하나 구현하는 경우 - 복잡하고 속도도 느린 편임\n",
    "import torch\n",
    "\n",
    "input_ids = tokenizer(\"지난해 7월 \", return_tensors='pt').to(model.device).input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        next_token = model(input_ids).logits[0, -1:].argmax(-1)\n",
    "        input_ids = torch.cat((input_ids[0], next_token), -1).unsqueeze(0)\n",
    "\n",
    "print(tokenizer.decode(input_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d1bfc-db36-465e-a30f-da77de6ffac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c20808-0161-420d-8137-e47b33cd007c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

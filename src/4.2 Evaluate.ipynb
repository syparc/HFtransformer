{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdacad-f5b3-4530-8eb2-a453baae52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# 학습 이후 평가 과정에서는 학습 단위마다 모델 손실이 출력됨\n",
    "# 그러나 이것은 prediction과 label의 차이를 계산한것일 뿐, scaling(범주화)되지는 않음\n",
    "# 객관적 평가를 위해서는 평가 지표를 사용\n",
    "#    -> 태스크별로 다른 평가 지표를 사용하며, 각각 범위가 정해져 있으므로 객관화 점수 평가가 용이함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f119f8-d69b-4e21-b21a-fbbaf0a37049",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Evaluate는 허깅페이스에서 제공하는 평가 지표 사용을 위한 라이브러리임\n",
    "# 일반적으로 사용가능한 평가 지표들\n",
    "#   > 분류 task : accuracy, f1 score, precision, recall\n",
    "#   > 생성 task : BLEU(BiLingual Evaluation Understudy), ROUGE(Recall-Oriented Understudy for Gisting Evaludation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e64ca1-5528-44c0-8c4b-db0ee021b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 07:17:14.867253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 07:17:14.874002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749626234.882345   21447 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749626234.884808   21447 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749626234.891133   21447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749626234.891143   21447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749626234.891144   21447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749626234.891145   21447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 07:17:14.893440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n",
      "{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n",
      "{'accuracy': 0.5, 'f1': 0.5, 'precision': 0.5, 'recall': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# 평가지표 하나 불러오기\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "# 평가지표 여러 개 불러오기\n",
    "metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "# 값 한꺼번에 입력하여 계산\n",
    "print(metrics.compute(predictions=[1,0,0,1], references=[0,1,0,1]))\n",
    "\n",
    "# add로 값을 저장한 후 한꺼번에 compute로 계산\n",
    "for y, pred in zip([0,1,0,1], [1,0,0,1]):\n",
    "    metrics.add(predictions=pred, references=y)\n",
    "print(metrics.compute())\n",
    "\n",
    "# add-batch로 배치 단위로 데이터 올리기\n",
    "for y,preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "    metrics.add_batch(predictions=preds, references=y)\n",
    "print(metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e531bab-61e4-45d4-90d8-7bd4bbc47b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 메트릭 만들기\n",
    "# 커스텀 메트릭 예시\n",
    "# dictionary 형태로 반환되는 구조의 함수라면 Trainer 클래스 매개변수 중 compute_metrics에 입력하여 사용 가능\n",
    "def simple_accuracy(preds, labels):\n",
    "    return {\"accuracy\": (preds==labels).to(float).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1786cf97-ea7f-4bca-899e-14d2b2051163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 메트릭 예시 - micro f1 : 긱 클래스의 f1 스코어를 계산한 후 평균값을 최종 f1 스코어로 활용\n",
    "def custom_metrics(pred):\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    labels=pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    return f1.compute(predictions=preds, references=labels, average='micro')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5f61cb-4eef-4011-9837-770d276d292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 45678\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 9107\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21447/1899012107.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "                            AutoTokenizer, \n",
    "                            AutoModelForSequenceClassification,\n",
    "                            Trainer,\n",
    "                            TrainingArguments,\n",
    "                            default_data_collator\n",
    "                        )\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    result = tokenizer(\n",
    "        sample[\"title\"],\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return result\n",
    "\n",
    "datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"guid\",\"title\",\"url\",\"date\"]\n",
    ")\n",
    "print(datasets)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"/home/ubuntu/model_path/evaluate/logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    output_dir=\"/home/ubuntu/model_path/evaluate/outputs\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=custom_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7d173e-bb2b-44e3-91c8-5985dcf3f924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 09:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.093500</td>\n",
       "      <td>0.761791</td>\n",
       "      <td>0.764687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.560452</td>\n",
       "      <td>0.831009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.437400</td>\n",
       "      <td>0.516203</td>\n",
       "      <td>0.838146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.501851</td>\n",
       "      <td>0.837268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.483207</td>\n",
       "      <td>0.849017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.496619</td>\n",
       "      <td>0.837048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.503887</td>\n",
       "      <td>0.836390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.458163</td>\n",
       "      <td>0.849566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.438374</td>\n",
       "      <td>0.852860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.425681</td>\n",
       "      <td>0.855935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5030346031188965, metrics={'train_runtime': 577.7653, 'train_samples_per_second': 13.846, 'train_steps_per_second': 0.865, 'total_flos': 2104982937600000.0, 'train_loss': 0.5030346031188965, 'epoch': 0.17513134851138354})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4869d-2597-48d1-816f-e6bbd80ae282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

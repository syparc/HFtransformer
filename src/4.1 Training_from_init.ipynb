{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1532bec-247f-40f4-9aef-09256fb63ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# 모델을 사전학습할 경우, 실제는 더 많은 데이터가 사용되기는 하지만, 유사한 순서로 진행됨\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c6549d-f3d0-4152-9213-a05f5a376746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"klue\",\"ynat\")\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4ed175-5c9d-4b0f-8695-8c612886ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 vailidation 데이터셋 title을 줄바꿈 단위로 파일에 저장. \n",
    "# 메모리를 낭비하지 않기 위해 파일 저장 후 불러와 쓰는 습관을 들이는 것이 좋음\n",
    "\n",
    "target_key = \"title\"\n",
    "for key in dataset.column_names.keys():\n",
    "    with open(f\"/home/ubuntu/model_path/tokenizer_data_{key}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(dataset[key][target_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd41c5-e70a-47bb-968b-cd53e0b8fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "#  Tokenizer Training\n",
    "#\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1df1f4-6b85-4259-8ac7-4b0671bcfc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '[UNUSED0]', '[UNUSED1]', '[UNUSED2]', '[UNUSED3]', '[UNUSED4]']\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 정의 - BERT모델에서 주로 사용하는 특수 토큰을 포함하도록 실습\n",
    "# 토크나이저 학습중에는 사용하지 않고 모델의 기술적 부분을 위해 필요\n",
    "\n",
    "user_defined_symbols = [\n",
    "    \"[PAD]\", # 문장 길이를 맞추기 위해 사용\n",
    "    \"[UNK]\", # 토크나이저가 인식할 수 없는 토큰\n",
    "    \"[CLS]\", # BERT 계열에서 문장 전체 정보를 저장하는 토큰\n",
    "    \"[SEP]\", # BERT 계열에서 문장 구분을 위해 사용하는 토큰\n",
    "    \"[MASK]\", # Masked LM에서 토큰 마스킹을 위해 사용하는 토큰\n",
    "]\n",
    "\n",
    "unused_token_num = 100\n",
    "unused_list = [f\"[UNUSED{i}]\" for i in range(100)] # 사전학습시 어휘에 없는 토큰 추가를 위한 빈공간\n",
    "\n",
    "whole_user_defined_symbols = user_defined_symbols + unused_list\n",
    "\n",
    "print(whole_user_defined_symbols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0614fe1-baa7-4740-b379-b8c5aaf988b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell how are u? '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "# 우선 transformer 라이브러리 대신 tokenizer 라이브러리 사용\n",
    "# WordPiece라는 규칙만 지정된, 빈 상태의 토크나이저 선언\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 정규화 진행\n",
    "from tokenizers import normalizers\n",
    "normalizer = normalizers.BertNormalizer()\n",
    "bert_tokenizer.normalizer = normalizer\n",
    "\n",
    "normalizer.normalize_str(\"Hell how\\nare u? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6faeef47-b7f4-47ba-a9c0-b384269c122d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안녕하세요', (0, 5)),\n",
       " ('.', (5, 6)),\n",
       " ('제대로', (7, 10)),\n",
       " ('인코딩이', (11, 15)),\n",
       " ('되는지', (16, 19)),\n",
       " ('확인', (20, 22)),\n",
       " ('중입니다', (23, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wordpiece 이전에 먼저 적용되는 토크나이저 \n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "pre_tokenizer = Whitespace()\n",
    "bert_tokenizer.pre_tokenizer = pre_tokenizer\n",
    "pre_tokenizer.pre_tokenize_str(\"안녕하세요. 제대로 인코딩이 되는지 확인 중입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bf8d93-5191-40d1-8b50-d035af91d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장이 인코딩되었을 때 취해야 할 형식 작성\n",
    "# BERT 모델이 요구하는 형식 - 가장 앞에 [CLS] 토큰이 있어야 하고, 문장 구별을 위해서는 [SEP] 토큰이 사용됨\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A $B:1 [SEP]:1\",\n",
    "    special_tokens=[(t,i) for i,t in enumerate(user_defined_symbols)],\n",
    ")\n",
    "\n",
    "bert_tokenizer.post_processor=post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c03d72-16cf-42f6-a441-d7d2b53633ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 학습을 위한 trainer 생성\n",
    "\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "vocab_size = 24000\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=whole_user_defined_symbols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9ccef5-5733-4285-8ed1-d56a6c875928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 수행\n",
    "from glob import glob\n",
    "\n",
    "bert_tokenizer.train(glob(f\"/home/ubuntu/model_path/*.txt\"), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980255eb-f399-4ab3-af2e-d5d279a8ba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 675, 906, 2220, 4518, 1240, 906, 2220, 569, 6985, 905, 6727, 6464, 586, 1881, 4308, 106, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'인 ##코 ##딩 및 디 ##코 ##딩 ##이 학습 후 제대로 되는 ##지 확인 ##합니다 .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 결과 확인\n",
    "\n",
    "output = bert_tokenizer.encode(\"인코딩 및 디코딩이 학습 후 제대로 되는지 확인합니다.\")\n",
    "print(output.ids)\n",
    "\n",
    "bert_tokenizer.decode(output.ids) # 디코딩 방법이 적용되지 않았으므로 #등의 문자가 나오는 정상적인 오류 상태가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761c3bfa-97ff-493c-bda4-bfadb248f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인코딩 및 디코딩이 학습 후 제대로 되는지 확인합니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 설정\n",
    "from tokenizers import decoders\n",
    "\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49e4cbff-7aca-4f24-bfae-805008096ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 675, 906, 2220, 4518, 1240, 906, 2220, 569, 6985, 905, 6727, 6464, 586, 1881, 4308, 106, 3]\n",
      "[CLS] 인코딩 및 디코딩이 학습 후 제대로 되는지 확인합니다. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 학습한 토크나이저를 Transformers 라이브러리로 옮기기\n",
    "# 토크나이저를 불러올 때 from_pretrained 대신 초기화(__init__)함수를 사용해 생성 - 이때 tokenizer_object 파라미터에 토크나이저 입력\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "fast_tokenizer = BertTokenizerFast(tokenizer_object=bert_tokenizer)\n",
    "encoded = fast_tokenizer.encode(\"인코딩 및 디코딩이 학습 후 제대로 되는지 확인합니다.\")\n",
    "decoded = fast_tokenizer.decode(encoded)\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fe6ccdc-e615-4b4a-82ee-b31c240bf4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/model_path/tokenizer_config.json',\n",
       " '/home/ubuntu/model_path/special_tokens_map.json',\n",
       " '/home/ubuntu/model_path/vocab.txt',\n",
       " '/home/ubuntu/model_path/added_tokens.json',\n",
       " '/home/ubuntu/model_path/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer 라이브러리에서 사용하는 형식대로 모델 저장\n",
    "output_dir = \"/home/ubuntu/model_path\"\n",
    "fast_tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd0617d6-eb25-4d19-a5cf-7d22229e82c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[2, 675, 906, 2220, 1675, 6464, 586, 1881, 3], [2, 18632, 1594, 22573, 6985, 3782, 3]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]\n",
      "[CLS] 인코딩 잘 되는지 확인 [SEP]\n",
      "[CLS] 안되면 다시 처음부터 학습하자 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 저장한 토크나이저 다시 불러와서 테스트\n",
    "new_tokenizer = BertTokenizerFast.from_pretrained(output_dir)\n",
    "encoded = new_tokenizer([\"인코딩 잘 되는지 확인\", \"안되면 다시 처음부터 학습하자\"])\n",
    "\n",
    "for k,v in encoded.items():\n",
    "    print(k,v)\n",
    "\n",
    "print(new_tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "print(new_tokenizer.decode(encoded[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf738f0-f1db-4607-980c-0fe3037012a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "#  Model Training\n",
    "#\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea3251e-3be1-411d-bf26-c41b5b91076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 토크나이저 불러오기\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "tokenizer_path = \"/home/ubuntu/model_path\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e707e24c-80a3-4cda-928a-f92796cb6d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 24000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 최초 선언에 필요한 config 준비 \n",
    "# embedding size, hidden size, num layers등 모델의 전반적 구조 정보가 저장됨\n",
    "# 다른 정보는 base 모델을 따라가되, vocab 정보는 맞추어줘야 함\n",
    "from transformers import BertConfig\n",
    "\n",
    "mycfg=BertConfig(vocab_size=tokenizer.vocab_size)\n",
    "print(mycfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e1234ff-b8b5-4f3f-b5fb-729718b3f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 24000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config에 따른 모델 선언\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model=BertForMaskedLM(mycfg)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1343a2e-1884-4ecb-b770-b9a3497c4e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

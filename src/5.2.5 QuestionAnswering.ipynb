{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82de302-2ed5-4312-b32e-743cc2ee92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 텍스트와 질문을 모델에 입력하고 그에 대한 답변을 하도록 함\n",
    "# 기계 톡해 이해(machine reading comprehension, MRC)의 하위 카테고리로 분류 가능\n",
    "# 답변 과정은 (주어진 context에서)추출(Extractive)과 생성(Generate)(context참고하여 새로 작성)의 두 가지로 나뉜다\n",
    "# 인코더형은 추출에, 디코더형은 생성에 강한 편임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bea0c0-c9a0-43cc-b0c0-af9231c9765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 04:58:17.313907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-18 04:58:17.320903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750222697.329130   40800 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750222697.331587   40800 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750222697.337952   40800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750222697.337961   40800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750222697.337963   40800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750222697.337963   40800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-18 04:58:17.340630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = 'klue/bert-base'  #인코더 기반 모델임\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# 모델에 레이블 개수 지정하지 않음. 기본적으로 추출 방식으로, 답변에 해당하는 부분의 시작과 끝 인덱스를 출겨하는 방식이기 때문임.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name) \n",
    "\n",
    "model # 맨 끝에 qa_outputs 레이어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6714ee7-814d-46f2-8c59-99664c8d1f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b83d26525444f34822c31866abcd919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be66987a4c149fd9f2459a95bd41242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a2088de7a14fa29c991d932a1970b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ee596c7eb4db9bde670969b72f053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내용: 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n",
      "질문: 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\n",
      "답변: {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"mrc\")\n",
    "sample = dataset['train'][0]\n",
    "\n",
    "print(f\"내용: {sample['context'][:50]}\")\n",
    "print(f\"질문: {sample['question']}\")\n",
    "print(f\"답변: {sample['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e21959-bcc6-4435-8f92-24b49e60f9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf7b1ff85354805a12151df92e0b8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec06068c63d42699df5e2e4cc0a7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 전 전처리 \n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]  # 문장 앞뒤 공백 제거\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",    # 일부 데이터는 최대 입력 길이를 초과할 수 있으므로 두번째 문장에 대해서만 잘라내도록 처리\n",
    "        return_offsets_mapping=True, # 인코딩된 토큰의 원문장내 위치를 알 수 있도록 인덱스 반환 \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions=[]\n",
    "    end_positions=[]\n",
    "\n",
    "    #답변의 시작과 끝을 원 context에 매핑\n",
    "    for i,offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i) # 문장내 context의 인덱스 범위를 확인\n",
    "\n",
    "        # Context 시작,끝 찾기\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] !=1 :\n",
    "            idx+=1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] ==1 :\n",
    "            idx+=1\n",
    "        context_end = idx -1\n",
    "\n",
    "        # 답변이 context 내에 포함되지 않으면 레이블 (0,0)으로 지정\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx<=context_end and offset[idx][0]<=start_char:\n",
    "                idx+=1\n",
    "            start_positions.append(idx-1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx>=context_start and offset[idx][1]>=end_char:\n",
    "                idx-=1\n",
    "            end_positions.append(idx+1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997653ff-5b61-4350-8ee5-630e26369f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  1174, 18956,  ...,  2170,  2259,     3],\n",
       "         [    2,  3920, 31221,  ...,  8055,  2867,     3],\n",
       "         [    2,  8813,  2444,  ...,  3691,  4538,     3],\n",
       "         ...,\n",
       "         [    2,  6860, 19364,  ...,  2532,  6370,     3],\n",
       "         [    2, 27463, 23413,  ..., 21786,  2069,     3],\n",
       "         [    2,  3659,  2170,  ...,  2470,  3703,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'start_positions': tensor([260,  31,   0,  80,  72,  81, 216, 348, 323, 348]),\n",
       " 'end_positions': tensor([263,  33,   0,  81,  78,  87, 221, 352, 328, 353])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 본래는 padding 처리 해야하지만, 이 경우 데이터셋이 이미 padding 처리가 되어 있음\n",
    "# 이에 데이터 타입과 차원만 맞춰주는 DefaultDataCollator 사용\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(10)])\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ee47ba-c74d-47f4-a93b-5aef0147908d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전에 불러온 klue/bert-base 모델 활용\n",
    "with torch.no_grad():\n",
    "    outputs=model(**batch)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = batch['input_ids'][0,answer_start_index:answer_end_index+1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a79de5-35bc-463b-b2d3-ae1f519ff0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.load(\"squad\")로 질의응답 평가 진행이 가능하지만, 후처리와 시간이 너무 많이 소요하므로 learnign loss로 대체"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

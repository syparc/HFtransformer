{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c27552-3434-4661-90eb-fc1ca21ff5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개 입력이 주어졌을 때 주어진 문장 중 옳은 것을 고르는 문제 -> 객관식\n",
    "# 프롬프트+답변 쌍을 여러개 받아 그중 정답을 출력함\n",
    "# 문장이 길수록 효과적임 -> 각 문장이 짧다면 모든 선택지를 한문장에 넣고 문장 분류를 하는 것이 더 효율적일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdafc88-2cfd-417b-a67a-c40811d2c3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 05:00:42.781631: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 05:00:42.790423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749704442.799006   24547 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749704442.801507   24547 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749704442.807815   24547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749704442.807826   24547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749704442.807827   24547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749704442.807828   24547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 05:00:42.810116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name) # num_labels 설정하지 않음\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d6cd36-798c-46e3-bdca-36ba25e27033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '<보기>를 참고하여 밑줄 친 두 단어의 의미를 한 단어에 담아\\n표현한 것으로 적절하지 않은 것은?\\n<보기>\\n그는 손으로 방문을 <word start>세게 밀었다.<word end> (⇒밀쳤다)\\n어제는 서쪽 하늘이 <word start>몹시 붉었다.<word end> (⇒붉디붉었다)', 'context': None, 'option#1': '그 집은 <word start>매우 크다. <word end>(⇒커다랗다)', 'option#2': '그는 건강을 <word start>다시 찾았다. <word end>(⇒되찾았다)', 'option#3': '그는 남의 말을 <word start>몰래 들었다. <word end>(⇒엿들었다)', 'option#4': '그는 계단에서 발을 <word start>잘못 디뎠다. <word end>(⇒헛디뎠다)', 'option#5': '그는 오늘 친구와 <word start>심하게 싸웠다. <word end>(⇒싸움질했다)', 'gold': 5, 'category': 'N/A', 'human_performance': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HAERAE-HUB/csatqa\", \"full\") # 수능 국어문제 - 오지선다형\n",
    "print(dataset[\"test\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec44f98-8df4-4756-a968-f0f329d41261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29705662b12544cc95bdb2db7e21ab4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ending_names = [\"option#1\", \"option#2\", \"option#3\", \"option#4\", \"option#5\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences=[\n",
    "        [context] * 5 for context in examples[\"context\"]   # 모든 문제가 공유하는 보기 항목인 context칼럼을 first_sentences로 선언\n",
    "    ]\n",
    "    question_headers = examples[\"question\"]  # 질문인 question 칼럼을 question_headers변수로 옮기기\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i,header in enumerate(question_headers) # 질문-답변 5개 결합 세팅\n",
    "    ]\n",
    "\n",
    "    # 토큰화를 위한 1차 flatten\n",
    "    first_sentences = sum(first_sentences,[])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # None 데이터 처리\n",
    "    first_sentences = [i if i else '' for i in first_sentences]\n",
    "    second_sentences = [i if i else '' for i in second_sentences]\n",
    "\n",
    "    # 토큰화\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "\n",
    "    # 토큰화 후 2차원 재배열\n",
    "    result = {\n",
    "        k: [v[i:i+5] for i in range(0,len(v),5)] for k,v in tokenized_examples.items()\n",
    "    }\n",
    "\n",
    "    # collator 사용 편하게 하기 위한 변수명 이동 -> 0번부터 시작하도록 변경\n",
    "    result['labels'] = [i-1 for i in examples['gold']]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['test'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f71db9-01a8-4bb0-b2e4-3b8902819cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '<보기>를 참고하여 밑줄 친 두 단어의 의미를 한 단어에 담아\\n표현한 것으로 적절하지 않은 것은?\\n<보기>\\n그는 손으로 방문을 <word start>세게 밀었다.<word end> (⇒밀쳤다)\\n어제는 서쪽 하늘이 <word start>몹시 붉었다.<word end> (⇒붉디붉었다)',\n",
       " 'context': None,\n",
       " 'option#1': '그 집은 <word start>매우 크다. <word end>(⇒커다랗다)',\n",
       " 'option#2': '그는 건강을 <word start>다시 찾았다. <word end>(⇒되찾았다)',\n",
       " 'option#3': '그는 남의 말을 <word start>몰래 들었다. <word end>(⇒엿들었다)',\n",
       " 'option#4': '그는 계단에서 발을 <word start>잘못 디뎠다. <word end>(⇒헛디뎠다)',\n",
       " 'option#5': '그는 오늘 친구와 <word start>심하게 싸웠다. <word end>(⇒싸움질했다)',\n",
       " 'gold': 5,\n",
       " 'category': 'N/A',\n",
       " 'human_performance': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리후 결과\n",
    "dataset[\"test\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e33c81-d97d-4e15-9cab-30c37119a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# 다중 분류 태스크에서는 DataCollatorWithPadding을 사용하기 어려우므로 Collator 직접 작성\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "        tokenizer: PreTrainedTokenizerBase\n",
    "        padding: Union[bool, str, PaddingStrategy] = True\n",
    "        max_length: Optional[int] = None\n",
    "        pad_to_multiple_of: Optional[int] = None\n",
    "        \n",
    "        def __call__(self, features):\n",
    "          label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "          labels = [feature.pop(label_name) for feature in features]\n",
    "        \n",
    "          batch_size = len(features)\n",
    "          num_choices = len(features[0][\"input_ids\"])\n",
    "        \n",
    "          flattened_features = [\n",
    "              [\n",
    "                  {k: v[i] for k, v in feature.items()}\n",
    "                  for i in range(num_choices)\n",
    "              ]\n",
    "              for feature in features\n",
    "          ]\n",
    "          flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "          batch = self.tokenizer.pad(\n",
    "              flattened_features,\n",
    "              padding=self.padding,\n",
    "              max_length=self.max_length,\n",
    "              pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "              return_tensors=\"pt\",\n",
    "          )\n",
    "        \n",
    "          batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "          batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "          return batch\n",
    "        \n",
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "batch = collator([tokenized_dataset[\"test\"][i] for i in range(5)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95629a6a-9c76-4eb0-8885-7a04c331dd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1068,  0.1716,  0.2760,  0.1267,  0.0006],\n",
       "        [ 0.3336,  0.3597,  0.3490,  0.3621,  0.3288],\n",
       "        [ 0.3613,  0.3684,  0.3666,  0.3498,  0.3922],\n",
       "        [ 0.4193,  0.4347,  0.4649,  0.4608,  0.1217],\n",
       "        [-0.0868, -0.0586, -0.1727, -0.1443, -0.1152]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론\n",
    "with torch.no_grad():\n",
    "    logits = model(**batch).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec5830ce-3e01-44f2-ab1a-d25f09b56f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 2 1]\n",
      "[4 4 0 3 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.2}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가 지표\n",
    "import evaluate\n",
    "\n",
    "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
    "print(pred_labels)\n",
    "true_labels = batch[\"labels\"].numpy()\n",
    "print(true_labels)\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1.compute(predictions=pred_labels, references=true_labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c0620-90fe-4fdf-add1-96faa413fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "??????????????????????//\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [ feature.pop(label_name) for feature in features ]\n",
    "                   \n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "\n",
    "        flattened_features = [\n",
    "            [\n",
    "                {k: v[i] for k,v in feature.item()}\n",
    "                for i in range(num_choices)\n",
    "            ]\n",
    "            for feature in features\n",
    "        ]\n",
    "        \n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch= self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -a) for k,v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "batch = collator([tokenized_dataset['test'][i] for i in range(5)])\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861e890c-d61a-429e-babf-bd9205778ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "dataset[\"test\"][10].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59ae33-8ae9-48ed-aa66-0f0e0ccbda01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3efbd5-af5b-4cd0-8f0c-79a25211a0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

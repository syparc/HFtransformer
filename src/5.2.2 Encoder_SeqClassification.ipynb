{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916a4370-9740-483c-9b62-27919ac6f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 Sequence Classification\n",
    "# 보통 2개로 분류는 이진 불류, 3개 이상은 다중 분류 multiclass classification이라고 부름\n",
    "\n",
    "# 2차원 문장 임베딩에서 1개 클래스를 고르는 1차원 확률분포를 반환함\n",
    "# 이를 위해 모델은 고차원->저차원 데이터 압축하는 풀링pooling 작업 진행 -> reduce-sum,reduce-mean 등\n",
    "\n",
    "# transformer encoder 모델은 문장의 모든 정보를 하나의 토큰 벡터에 저장하도록 학습 -> 문장 맨 앞의 [CLS] 토큰 -> classification을 뜻함\n",
    "# [CLS]는 문자으이 맨 앞에 삽입되어 분류 태스크 또는 BOS(Begin Of Sentence) 표시에 사용되고 나머지 태스트에서는 대부분 무시됨\n",
    "# [CLS] 토큰 벡터를 입력으로 받는 FFNN(FeedForward Neural Network)을 추가로 부착하여 미세조정 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e90d43-77b8-43fc-a897-fd05b273d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 04:29:40.828111: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 04:29:40.835027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749702580.843338   23332 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749702580.845812   23332 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749702580.852134   23332 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749702580.852143   23332 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749702580.852144   23332 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749702580.852145   23332 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 04:29:40.854843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # 미세조정되지 않은 상태이므로 설정값 추가해 주어야 함\n",
    "\n",
    "model # embedding, encoder, pooling, classifier등의 레이어 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4303e7-b4db-45b6-a3f9-67e678b12498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델이 지닌 클래스 확인\n",
    "print(model.config.id2label)\n",
    "\n",
    "# 정상 작동 확인\n",
    "inputs = tokenizer(\"안녕? 내 강아지는 귀여워.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3987a55-6a1b-4224-8140-3d90638fb067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0543183efa4b208b681817a77f410b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454af2e32001490eb5e44f681fd43a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/68.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa41b2e76f04835aa7040a958120268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd737cdf5d24ab19d119d567bb1f4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'source', 'sentence1', 'sentence2', 'labels'],\n",
       "    num_rows: 11668\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\",\"sts\")\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662feb75-03d9-4dd7-803a-7a1281d028f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cacf6766a2e4e5c8495365ab6923944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f6e6499bcf463e9540f592f761a23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 전처리 함수 정의\n",
    "def process_data(batch):\n",
    "    result = tokenizer(batch[\"sentence1\"], text_pair=batch[\"sentence2\"])\n",
    "    result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n",
    "    return result\n",
    "\n",
    "# 배치단위 전처리\n",
    "dataset = dataset.map(\n",
    "    process_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720cbeb2-ee1f-4994-bd4d-86033407b896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0021, -0.2343],\n",
       "        [ 0.1941,  0.1877],\n",
       "        [-0.0738, -0.3436],\n",
       "        [-0.2128,  0.2313],\n",
       "        [ 0.2154, -0.1252],\n",
       "        [ 0.0528,  0.3561],\n",
       "        [ 0.2005, -0.3857],\n",
       "        [ 0.0593,  0.0934],\n",
       "        [ 0.4533, -0.3088],\n",
       "        [ 0.0271, -0.4454]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 길이 통일 with padding - 테스트용 10개 입력\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "batch = collator([dataset[\"train\"][i] for i in range(10)])\n",
    "\n",
    "# 10개 값에 대해 추론\n",
    "with torch.no_grad():\n",
    "    logits = model(**batch).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb25f39e-eff0-48e0-9541-63bb4e43614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 0 1 0 0]\n",
      "[1 0 0 0 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 추론한 10개 레이블과 실제 레이블 정의\n",
    "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
    "print(pred_labels)\n",
    "\n",
    "true_labels = batch[\"labels\"].numpy()\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15134246-b8bd-4ee0-862a-6df9e89ef3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가지표\n",
    "import evaluate\n",
    "\n",
    "# F1 score\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1.compute(predictions=pred_labels, references=true_labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ff94b0-c5d7-4c02-9341-deeee8ca6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 분류 모델 헤더는 softmax 함수 적용 전의 logit값을 반환함. 즉, 스케일링 적용 안되어있음\n",
    "# logit 함수가 무한대 버우이를 가지므로, 이 특성을 이용하면 SequenceClassification 모델을 회귀 모델로 사용 가능\n",
    "# label값을 1로 설정하면 모델은 회귀 태스크로 인식 -> 손실함수를 CE(Cross-Entropy) 대신 MSE(mean Square Error)를 사용함 -> 출력차원은 1로 설정됨\n",
    "# 회귀 문제는 주로 MSE나 MAE(Meas Absolute Error)를 손실함수로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafa135d-f117-4c13-b651-b370c55abd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, BertModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd412a-1674-47d7-8f95-6175a69906fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
